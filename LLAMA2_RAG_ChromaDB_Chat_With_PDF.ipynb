{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMqcBBG33rMEe5mzOVPovcM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sravan1320/RAG/blob/main/LLAMA2_RAG_ChromaDB_Chat_With_PDF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "pHfpQy5EouJk"
      },
      "outputs": [],
      "source": [
        "# !pip install transformers langchain torch constants chromadb accelerate sentence-transformers pypdf pdfminer.six"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pyGCS7w5uI0t",
        "outputId": "1003dddb-2317-4540-92b7-19d6a576a35f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n",
            "    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Token: \n",
            "Add token as git credential? (Y/n) N\n",
            "Token is valid (permission: write).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from chromadb.config import Settings\n",
        "CHROMA_SETTINGS = Settings(\n",
        "        chroma_db_impl='duckdb+parquet',\n",
        "        persist_directory='db',\n",
        "        anonymized_telemetry=False\n",
        ")"
      ],
      "metadata": {
        "id": "x1O8WtEJxrIt"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##loading LLAMA model"
      ],
      "metadata": {
        "id": "I_B7cnKV1mGd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "# import torch\n",
        "# checkpoint = \"MBZUAI/LaMini-T5-738M\"\n",
        "# print(f\"Checkpoint path: {checkpoint}\")  # Add this line for debugging\n",
        "# tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "# base_model = AutoModelForSeq2SeqLM.from_pretrained(\n",
        "#     checkpoint,\n",
        "#     torch_dtype=torch.float32\n",
        "# )"
      ],
      "metadata": {
        "id": "24XVL8XDywKr"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "id": "xJ9-4zQ_3-iO"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define Embedding Generate Function\n"
      ],
      "metadata": {
        "id": "l8Q1VGxz1tOz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from langchain.embeddings import HuggingFaceEmbeddings\n",
        "\n",
        "# # specify embedding model (using huggingface sentence transformer)\n",
        "# embedding_model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
        "# model_kwargs = {\"device\": \"cuda\"}\n",
        "# embeddings = HuggingFaceEmbeddings(model_name=embedding_model_name, model_kwargs=model_kwargs)\n"
      ],
      "metadata": {
        "id": "bAQl_Jvx2I8W"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import PyPDFLoader, DirectoryLoader, PDFMinerLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings import SentenceTransformerEmbeddings, HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "import os\n",
        "# from constants import CHROMA_SETTINGS\n",
        "\n",
        "\n",
        "persist_directory = \"db\"\n",
        "\n",
        "for root, dirs, files in os.walk(\"docs\"):\n",
        "    for file in files:\n",
        "        if file.endswith(\".pdf\"):\n",
        "            print(file)\n",
        "            loader = PyPDFLoader(os.path.join(root, file))\n",
        "documents = loader.load()\n",
        "print(\"splitting into chunks\")\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
        "texts = text_splitter.split_documents(documents)\n",
        "#create embeddings here\n",
        "print(\"Loading LlamaCppEmbeddings model\")\n",
        "# checkpoint = \"MBZUAI/LaMini-T5-738M\"\n",
        "# print(f\"Checkpoint path: {checkpoint}\")  # Add this line for debugging\n",
        "# embeddings = LlamaCppEmbeddings(model_name=checkpoint)\n",
        "# specify embedding model (using huggingface sentence transformer)\n",
        "checkpoint = \"sentence-transformers/all-mpnet-base-v2\"\n",
        "model_kwargs = {\"device\": \"cuda\"}\n",
        "embeddings = HuggingFaceEmbeddings(model_name=checkpoint, model_kwargs=model_kwargs)\n",
        "\n",
        "#create vector store here\n",
        "print(f\"Creating embeddings. May take some minutes...\")\n",
        "db = Chroma.from_documents(texts, embeddings, persist_directory=persist_directory) #, client_settings=CHROMA_SETTINGS)\n",
        "db.persist()\n",
        "db=None\n",
        "\n",
        "print(f\"Ingestion complete! You can now run privateGPT.py to query your documents\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B7JytLjDue5-",
        "outputId": "d7da1fa6-e435-46c2-88d6-a5b10e92b3b4"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2018-Annual-Report.pdf\n",
            "splitting into chunks\n",
            "Loading LlamaCppEmbeddings model\n",
            "Creating embeddings. May take some minutes...\n",
            "Ingestion complete! You can now run privateGPT.py to query your documents\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Chroma Load from Disk Example"
      ],
      "metadata": {
        "id": "BGpUpAXgj5jJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load from disk\n",
        "query = \"what is quaterly results (unaudited)?\"\n",
        "db3 = Chroma(persist_directory=\"db\", embedding_function=embeddings)\n",
        "docs = db3.similarity_search(query)\n",
        "print(docs[0].page_content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TZwH7e0Aj1ui",
        "outputId": "3727d19b-c2d7-4f60-b977-55b9c0aaa96e"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Note 11â€”QUAR TERL YRESUL TS(UNAUDITED)\n",
            "Thefollowing tables contain selected unaudited statement ofoperations information foreach quarter of2017 and2018.\n",
            "Thefollowing information reflects allnormal recurring adjustments necessary forafairpresentation oftheinformation forthe\n",
            "periods presented. Theoperating results foranyquarter arenotnecessarily indicative ofresults foranyfuture period. Our\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import base64\n",
        "import time\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "from transformers import pipeline\n",
        "import torch\n",
        "import textwrap\n",
        "from langchain.document_loaders import PyPDFLoader, DirectoryLoader, PDFMinerLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings import SentenceTransformerEmbeddings, HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "device = torch.device('cuda')\n",
        "\n",
        "checkpoint = \"MBZUAI/LaMini-T5-738M\"\n",
        "print(f\"Checkpoint path: {checkpoint}\")  # Add this line for debugging\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "base_model = AutoModelForSeq2SeqLM.from_pretrained(\n",
        "    checkpoint,\n",
        "    device_map=device,\n",
        "    torch_dtype=torch.float32\n",
        ")\n",
        "\n",
        "persist_directory = \"db\"\n",
        "\n",
        "def llm_pipeline():\n",
        "    pipe = pipeline(\n",
        "        'text2text-generation',\n",
        "        model = base_model,\n",
        "        tokenizer = tokenizer,\n",
        "        max_length = 256,\n",
        "        do_sample = True,\n",
        "        temperature = 0.3,\n",
        "        top_p= 0.95\n",
        "    )\n",
        "    local_llm = HuggingFacePipeline(pipeline=pipe)\n",
        "    return local_llm\n",
        "\n",
        "def qa_llm():\n",
        "    llm = llm_pipeline()\n",
        "    embedding_checkpoint = \"sentence-transformers/all-mpnet-base-v2\"\n",
        "    model_kwargs = {\"device\": \"cuda\"}\n",
        "    embeddings = HuggingFaceEmbeddings(model_name=embedding_checkpoint, model_kwargs=model_kwargs)\n",
        "    # embeddings = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
        "    db = Chroma(persist_directory=\"db\", embedding_function = embeddings) #, client_settings=CHROMA_SETTINGS)\n",
        "    retriever = db.as_retriever()\n",
        "    qa = RetrievalQA.from_chain_type(\n",
        "        llm = llm,\n",
        "        chain_type = \"stuff\",\n",
        "        retriever = retriever,\n",
        "        return_source_documents=True\n",
        "    )\n",
        "    return qa\n",
        "\n",
        "def process_answer(instruction):\n",
        "    response = ''\n",
        "    instruction = instruction\n",
        "    qa = qa_llm()\n",
        "    generated_text = qa(instruction)\n",
        "    answer = generated_text['result']\n",
        "    return answer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AnyOCb9JtZkx",
        "outputId": "3f50ad47-07b9-4fad-c585-c4814616ef33"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checkpoint path: MBZUAI/LaMini-T5-738M\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"what is quaterly results (unaudited)?\"\n",
        "process_answer(query)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "x-G0vHhylu3e",
        "outputId": "974ea3a3-cb7d-4867-e54f-40e2bde72960"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The provided text does not provide information about what quartary results are.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "qa = qa_llm()\n",
        "generated_text = qa(query)\n",
        "print(generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b21ualD-l-9E",
        "outputId": "404cfb50-5478-4c4c-da06-d77f1db0590f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'query': 'what is quaterly results (unaudited)?', 'result': 'The provided context does not give a clear answer to what quaterly results (unaudited) are.', 'source_documents': [Document(page_content='Note 11â€”QUAR TERL YRESUL TS(UNAUDITED)\\nThefollowing tables contain selected unaudited statement ofoperations information foreach quarter of2017 and2018.\\nThefollowing information reflects allnormal recurring adjustments necessary forafairpresentation oftheinformation forthe\\nperiods presented. Theoperating results foranyquarter arenotnecessarily indicative ofresults foranyfuture period. Our', metadata={'page': 76, 'source': 'docs/2018-Annual-Report.pdf'}), Document(page_content='reliability offinancial reporting andthepreparation offinancial statements forexternal purposes inaccordance with generally\\naccepted accounting principles. Acompanyâ€™ sinternal control over financial reporting includes those policies andprocedures\\nthat(1)pertain tothemaintenance ofrecords that,inreasonable detail, accurately andfairly reflect thetransactions and\\ndispositions oftheassets ofthecompany; (2)provide reasonable assurance thattransactions arerecorded asnecessary topermit', metadata={'page': 78, 'source': 'docs/2018-Annual-Report.pdf'}), Document(page_content='subsequent periods. Weregularly assess thelikelihood ofanadverse outcome resulting from these proceedings todetermine the\\nadequacy ofourtaxaccruals. Although webelieve ourtaxestimates arereasonable, thefinal outcome ofaudits, investigations,\\nandanyother taxcontroversies could bematerially different from ourhistorical income taxprovisions andaccruals.\\n20', metadata={'page': 27, 'source': 'docs/2018-Annual-Report.pdf'}), Document(page_content='operating results.\\nWeMay Experience Significant Fluctuations inOurOperating Results andGrowth Rate\\nWemaynotbeabletoaccurately forecast ourgrowth rate.Webase ourexpense levels andinvestment plans onsales\\nestimates. Asignificant portion ofourexpenses andinvestments isfixed, andwemaynotbeabletoadjust ourspending\\nquickly enough ifoursales arelessthanexpected.\\nOurrevenue growth maynotbesustainable, andourpercentage growth rates maydecrease. Ourrevenue andoperating', metadata={'page': 13, 'source': 'docs/2018-Annual-Report.pdf'})]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TFUJGcy0nOHF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}